\documentclass[11pt]{scrartcl}

\usepackage[hidelinks, colorlinks=true]{hyperref}

\usepackage[english]{babel}
\usepackage{helvet}
\usepackage{mathpazo}
\usepackage{euler}
\usepackage{scrlayer-scrpage}
\usepackage[authoryear]{natbib}
\usepackage{csquotes}
\usepackage{xcolor}
\definecolor{@darkblue}{RGB}{0,0,140}
\definecolor{@darkgreen}{RGB}{0,100,70}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = @darkblue, %Colour for external hyperlinks
	linkcolor    = @darkblue, %Colour of internal links
	citecolor    = @darkgreen %Colour of citations
}
\usepackage[all]{hypcap}
\usepackage{subcaption}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{adjustbox}

\usepackage{float}
\usepackage{mathtools}
\usepackage{epigraph}
\usepackage{setspace}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tikz, pgfplots}
\usetikzlibrary{calc,decorations.pathmorphing,patterns, pgfplots.colorbrewer, shapes, arrows}
% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cheap} = [rectangle,  
text width=12em, text centered, minimum height=4em, node distance=3cm]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
minimum height=2em]

\bibliographystyle{unsrt}

% ----------------- Layout stuff  -------------------------------

\pagestyle{scrheadings}
\clearscrheadfoot
%\automark{section}
%\renewcommand\sectionmark[1]{\markright{\MakeMarkcase {\thesection\hskip .5em\relax#1}}\rohead{\ifnum\expandafter\pdfstrcmp\botmark=0 \rightmark\else\leftmark{} --- \rightmark\fi}}


\ofoot{\pagemark}

% ---------------------  Title page setup ----------------------------
% Title Page
\title{Minimal Surfaces}
\author{add your matriculation numbers here!\\Chenfei Fan \\  Praveen Mishra \\ Sankarasubramanian Ragunathan\quad 389851\\ Philipp Schleich\quad 391779}
%
\subject{Report \\ Simulation Sciences Laboratory}
\date{\today \\ \vspace{0.9cm}}

\publishers{
	\vspace{2em}
	
	\begin{tabular}[!b]{ll}
		Supervisor: & Prof. Dr. Uwe Naumann \\[3pt]
		            & Klaus Leppkes
	\end{tabular}
%	\begin{figure}[h!]
%		\centering
%		\includegraphics[width=.6\linewidth]{figs/mathccesText.png}%
%		
%	\end{figure}
}
% -------------------------------------------------------------

\newcommand{\mSurf}[1]{\ensuremath{\mathcal{F}\left[#1\right]}}
\newcommand{\mSurfDisc}[1]{\ensuremath{\mathtt{F}^h\left[#1\right]}}
\newcommand{\Dx}[1]{\ensuremath{\mathtt{d}_x[#1]}}
\newcommand{\Dy}[1]{\ensuremath{\mathtt{d}_y[#1]}}
\newcommand{\Dxx}[1]{\ensuremath{\mathtt{d}_{xx}[#1]}}
\newcommand{\Dyy}[1]{\ensuremath{\mathtt{d}_{yy}[#1]}}
\newcommand{\Dxy}[1]{\ensuremath{\mathtt{d}_{xy}[#1]}}

\newcommand{\inv}{\ensuremath{^{-1}}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\rb}[1]{\left( #1 \right)}
\newcommand{\cb}[1]{\left \{ #1 \right \}}

\begin{document}
\maketitle

%\section*{Abstract}
%\begin{abstract}
%\noindent Abstract might be unnecessary
%\end{abstract}
%
\clearpage
\protect \tableofcontents



\newpage
	
\onehalfspacing
% #########################################################################
\section{Introduction}
\textit{A Minimal Surface is defined as a surface that has zero mean curvature}. A minimal surface is characterized as surface of minimal surface area for the given boundary conditions.For example, a plane is a trivial minimal surface.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{./figs/minsurfIntro.png}
	\caption{A \textit{Minimal Surface} with the local area vector that is to be minimized.}
\end{figure}
The minimal surface is obtained as a solution to the Lagrange's equation (also known as the \textit{Minimal-Surface Equation} (MSE)) given by:
\begin{align}
	(1 + z_{y}^{2})z_{xx} - 2z_{x}z_{y}z_{xy} + (1+z_{x}^{2})z_{yy} = \mathcal{F}[z] &= 0 && \text{in} \; \Omega \\
	\notag
	z(x,y) &= g(x,y) && \text{on} \; \partial \Omega
\end{align}
MSE is a Nonlinear Elliptic PDE that has a combination of first and second order differential terms in both $x$ and $y$. Being a nonlinear PDE, we do not readily obtain an analytical solution necessitating the use of numerical methods, such as linearization of MSE using Newton-Raphson iterations, to solve the problem. It is also important to bear in mind that as $z$ has second order derivative terms, we need $z \in \mathcal{C}^{2}$ which means that we need to provide smooth enough boundary conditions on $\partial \Omega$ in order to be able to compute the derivative terms in MSE.

MSE is used in designing structures that have great tensile strength and also to minimize the cost of construction as structural designs resulting from MSE require less material. MSE is also used to provide a relativistic description on the formation of Black Holes. Material lattice structures of biological organisms such as butterfly wings are minimal surface structures (Butterfly wings have a Gyroidal minimal surface). Hydrophobic  co-polymer structures also take the shape of a minimal surface in order to reduce the energy of the structure arising due to surface tension. 
% #########################################################################
\section{Background}
\subsection{General background (don't like this title)}
We look at surfaces in $ \mathbb{R}^3 $, defined over an open set $\Omega \subset \mathbb{R}^2$. 
The surface of desire should contain the least possible area among all possible surfaces, that assume given values on the boundary of $\Omega$, denoted by $\partial \Omega$. \cite{Sakai1976}

Lagrange showed in 1760, that such a surface is characterized by the graphic of a function $z(x,y)$, $z: \mathbb{R}^2 \to \mathbb{R} $, which is twice continuously differentiable on a two-dimensional domain, particularly in a subset of $\mathbb{R}^2$.
This function $z$ has to fulfill the so called \textit{Minimal-Surface Equation} (MSE) stated below.
\begin{align}\label{eq:MSE}
	(1+z_y^2) z_{xx} - 2 z_x z_y z_{xy} + (1+z_x^2)z_{yy} = \mSurf{z} &= 0 \quad &\text{in } \Omega \\
	z(x,y) &= g(x,y) \quad &\text{on } \partial \Omega \notag
\end{align}
Clearly, this formulation satisfies the prescribed boundary values given by $g(x,y)$ due to the Dirichlet boundary condition on $\partial\Omega$.
As to why the graphic of functions solving this equation describes a minimal surface, we refer to the literature. For example \cite{Sakai1976} gives a very straightforward proof.

In the following, we will call the differential operator $\mSurf{\cdot}$ the Minimal-Surface Operator (MSO). The resulting partial differential equation (PDE) in eq. \eqref{eq:MSE} turns out to be an elliptic PDE of second order, which is in particular \textit{non-linear}. The solution of such a PDE is not trivial, and typically requires numerical treatment. For certain cases, analytical descriptions are available, such as for Scherk's surface.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{figs/scherk-test}
	\caption{Scherk's first surface on $[0,1]^2$}\label{fig:scherk01}
\end{figure}


\newpage

As an example, Scherk's first surface (compare to figure \ref{fig:scherk01}) $\Sigma$ rescaled on $\Omega = [0,1]^2$ is given by
\begin{equation}
	\Sigma = \left\{ \left. \left(x, y, \log \left( \frac{\cos (\pi(x-\frac{1}{2}))}{\cos (\pi(y-\frac{1}{2}))} \right) \right) \in \mathbb{R}^{3} \right  | 0 < x, y < 1 \right\}.
\end{equation} 
which is the limit $n\to \infty$ of 
\begin{align}
\Sigma_n = \left\{ \left. \left(x, y, u_n(x,y) \right) \in \mathbb{R}^{3} \right  | 0 < x, y < 1 \right\}, \\
\lim\limits_{y\to \pm 1} \to n \quad, 0 \le x \le 1 \\
\lim\limits_{x\to \pm 1} \to -n \quad, 0 \le y \le 1.
\end{align}


The numerical solution of the MSE will require setting approbiate boundary conditions. Since $\log \left( \frac{\cos (\pi(x-\frac{1}{2}))}{\cos (\pi(y-\frac{1}{2}))} \right) \to \pm \infty $ on $\partial\Omega$, this is numerically not very practical. 

We thus introduce $\beta_x,\beta_y \in (0,1)$ s.th. 
\begin{equation}
\Sigma_{\text{trunc}} = \left\{ \left. \left(x, y, \log \left( \frac{\cos (\pi\beta_x(x-\frac{1}{2}))}{\cos (\pi\beta_y(y-\frac{1}{2}))} \right) \right) \in \mathbb{R}^{3} \right  | 0 < x, y < 1 \right\}.
\end{equation} 

By these means, we solve the MSE on $\Omega_\text{trunc}=[0,1]^2\subset\Omega$ with $\Omega=[0,\frac{1}{\beta_x}]\times[0,\frac{1}{\beta_y}]$. Exact boundary values on $\partial\Omega_\text{trunc}$ ensure the correctness of the results. We choose $\beta_x=\beta_y=\beta$, as we restrict ourselves to solving on a square domain.

Later on, we will use this surface as a test-case to verify our numerical results.

\newpage
\subsection{Numerical solution}
\subsubsection{Discretization}
\label{section:Discretization}
In this project, we are supposed to solve the MSE numerically on $\Omega \equiv (0,1)\times(0,1)$. In the following, discretized quantities are indicated by a superscript $h$. The spatial domain is to be discretized using a structured mesh with equidistant grid spacing both in $x,y$, i.e. we have the same number of grid points in both directions, $N=N_x=N_y$. Thus, we define $\Omega^h := \{ (x,y) \in \mathbb{R}^2:\text{ }(x,y) = (ih, jh), \text{ } 0 \le i,j < N,\text{ }hN=1\}$.

\begin{figure}[h!]
	\centering
	\adjustbox{scale=3}{ 
	\begin{tikzpicture}
	\draw [gray!50!white, step=0.2] (-0.3,-0.3) grid (1.3,1.3);
	\draw [thick, ->] (0,-0.4) -- (0,1.4);
	\draw [thick, ->] (-0.4,0) -- (1.4,0);
	
	\draw [red, thick] (0,0) rectangle (1,1);
	
	\node at (-0.17,-0.14) [scale=0.4] {(0,0)};		
	\node at (1.17,-0.14) [scale=0.4] {(1,0)};		
	\node at (-0.17,1.1) [scale=0.4] {(0,1)};		
	\node at (1.17,1.1) [scale=0.4] {(1,1)};	
	\node at (-.55, .7) [blue, scale=.5] {$\Omega^h$};	
	\node at (-.4, .45) [red, scale=.5] {$\partial\Omega^h$};	
	
	
	
	% Boundary nodes
	\foreach \x in {0,1,2,3,4,5} {
		\foreach \y in {0} { % bottom
			\node at (\x/5,\y/5) [circle,fill=red, scale=0.3] {};
			%this way circle of nodes will not be transformed
		}
	}
	\foreach \x in {0} { % left
		\foreach \y in {0, 1,2,3,4,5} {
			\node at (\x/5,\y/5) [circle,fill=red, scale=0.3] {};
			%this way circle of nodes will not be transformed
		}
	}
	\foreach \x in {0,1,2,3,4,5} { % top
		\foreach \y in {5} {
			\node at (\x/5,\y/5) [circle,fill=red, scale=0.3] {};
			%this way circle of nodes will not be transformed
		}
	}
	\foreach \x in {5} { % right
		\foreach \y in {0,1,2,3,4,5} {
			\node at (\x/5,\y/5) [circle,fill=red, scale=0.3] {};
			%this way circle of nodes will not be transformed
		}
	}
	%Inner nodes
	\foreach \x in {1,2,3,4} {
		\foreach \y in {1,2,3,4} {
			\node at (\x/5,\y/5) [circle,fill=blue, scale=0.3] {};
			%this way circle of nodes will not be transformed
		}
	}
	\end{tikzpicture}
	}
	\caption[Depiction of $\Omega^h$ with $N=5$, $hN=1$]{Depiction of $\Omega^h$ with $N=5$, $hN=1$. Blue: inner nodes, red: boundary nodes}
\end{figure}

We choose to discretize the MSO on $\Omega^h$ by Finite Differences, since this is usually the easiest way to go, and on a structured grid, would anyways yield similar discrete equation as in Finite Volume or Finite Element methods.

To obtain a second order consistent discrete MSO (\mSurfDisc{\cdot}), we use central difference stencils on the first, second and mixed derivative. Since all these stencils have make only use of immediate neighours, there is no need to treat nodes close to the boundary differently, since the boundary is given by $g(\cdot)$.

This way, we obtain a discrete version of \eqref{eq:MSE}:
\begin{align}
	\left(1+\Dx{z^h}^2\right)\Dyy{z^h}-\Dx{z^h} \Dy{z^h}\Dxy{z^h} + \left(1+\Dy{z^h}^2\right)\Dxx{z^h} = \mSurfDisc{z^h} &= 0  \quad \text{in } \Omega^h \notag \\
	z^h &= g \quad \text{on } \partial\Omega^h \label{eq:dMSE},
\end{align}
while the stencils defined on the inner nodes are given as follows ($1 \le i,j \le N-1$):
\begin{align}
	\Dx{z}_{i,j} &= \frac{z_{i+1,j}-z_{i-1,j}}{2h}\label{eq:dx} \\
	\Dy{z}_{i,j} &= \frac{z_{i,j+1}-z_{i,j-1}}{2h}\label{eq:dy} \\
	\Dxx{z}_{i,j} &= \frac{z_{i+1,j} - 2z_{i,j} + z_{i-1,j}}{h^2}\label{eq:dxx}\\
	\Dyy{z}_{i,j} &= \frac{z_{i,j+1} - 2z_{i,j} + z_{i,j-1}}{h^2}\label{eq:dyy}\\
	\Dxy{z}_{i,j} &= \frac{z_{i+1,j+1} + z_{i-1,j-1} - z_{i-1,j+1}-z_{i+1,j-1}}{4h^2}\label{eq:dxy}. 
\end{align}
Figure~\ref{fig:fdmmesh} depicts the stencils on a segment of $\Omega^h$.
Since the stencils have only support to the nearest neighbors, there is no need tho treat nodes close to the boundary any different. Furthermore, this gives already rise to the conclusion, that the associated matrix will be sparse (for each of the $N$ gridpoints, only 9 instead of $N$ partners contribute).

\begin{figure}
	\centering
	\includegraphics[width=.4\linewidth]{figs/FDMMesh}
	\caption{Snippet of the mesh with Finite Differences stencils\\red/blue: first and second order stencils in $x,y$\\
	green: mixed stencil}\label{fig:fdmmesh}
\end{figure}
\newpage
\subsubsection{Solution}
The main difficulty in solving the MSE lies in the non-linearity of $\mSurfDisc{\cdot}$. Since it is not possible, to directly invert for $z^h = \left(\mathtt{F}^h\right)\inv 0$, one needs to use a procedure such as Newton-Raphson iterations. The main idea is to use some initial guess $z^h_0$, while in general, $\mSurfDisc{z^h_0}=r^h_k\neq0$. The goal is then to generate a sequence of $z^h_k$ such that $r^h_k\to 0$ for increasing, but reasonably small $k$. Algorithm~\ref{algo:standard-newton} shows the standard Newton-Raphson procedure, that takes as input a specific initial guess (here: 0), a tolerance for convergence \textsc{tol} and the nonlinear operator $\mSurfDisc{\cdot}$, outputting an approximate solution to the MSE $z^h_{k_\text{fin}}$.
\begin{algorithm}
	\caption{Newton's method applied on the discrete MSE}\label{algo:standard-newton}
	\begin{algorithmic}
		\State $k \gets 0$ 
		\State $z^h_k \gets 0 $ 
		\State $r^h_k \gets \mSurfDisc{z^h_k}$
		\While{$\lVert r^h_k \rVert > \textsc{tol}$} \Comment{ We choose $ \lVert \cdot \rVert = \lVert \cdot \rVert_2  $}
			\State $z^h_{k+1} \gets z^h_{k} - \left(\nabla\mSurfDisc{z^h_k}\right)\inv r^h_k $
			\State $k \gets k+1$ 
			\State $r^h_k \gets \mSurfDisc{z^h_k}$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

This algorithm involves the computation of the inverse of the gradient of $\mSurfDisc{z^h_k}$ for each iteration $k$. Later, we will present and compare two different ways on how to contrive this.

It is well known, that the Newton-Raphson procedure yields fast convergence, but the success of this convergence is highly dependent on the initial guess $z^h_0$. Furthermore, convergence can be improved by choosing an initial guess that is closer to the solution. 
One possible initial guess is to take the average boundary value, \\$z^h_{0,\text{ave}}=\frac{1}{|\partial\Omega^h|} \sum_{(x_i,y_j)\in \partial\Omega^h} \big(g(x_i,y_j)\big)$. While this might help convergence for surfaces, that have a certain offset with respect to the $x,y$-plane, it does not provide any additional information (such as preshaping the correct curvature).

To obtain a more educated initial guess, recall the MSE, equation \eqref{eq:MSE}. Considering only linear terms, we get 
\begin{align}
	\mathcal{L} [z] = z_{xx} + z_{yy} &= 0 \quad\quad\quad\quad \text{ in } \Omega \\
	z(x,y) &= g(x,y) \quad \text{ on } \partial\Omega \notag 
\end{align}\newpage
This linear, second order PDE is well known as the Laplace-equation, and can be solved by discretizing the second derivatives using the definitions for $\Dxx{\cdot},\Dyy{\cdot}$ introduced before, yielding $\mathtt{L}[z^h]=0 $ with the usual boundary conditions. Since Laplace's equation can be regarded as a linearization of the MSE, we suspect to get a better initial guess (and thus faster and a more robust convergence behaviour) by first solving for $z^h_0= \mathtt{L}\inv 0$. This way, we obtain a slightly modified version of Algorithm~\ref{algo:standard-newton}, stated in Algorithm~\ref{algo:laplace-newton}.

\begin{algorithm}
	\caption{Newton's method using Laplace's Equ. as initial guess}\label{algo:laplace-newton}
	\begin{algorithmic}
		\State $k \gets 0$ 
		\State $z^h_k \gets \mathtt{L}\inv 0 $ \Comment{Solve Laplace's Equ. as initial guess}
		\State $r^h_k \gets \mSurfDisc{z^h_k}$
		\While{$\lVert r^h_k \rVert > \textsc{tol}$} 
		\State $z^h_{k+1} \gets z^h_{k} - \left(\nabla\mSurfDisc{z^h_k}\right)\inv r^h_k $
		\State $k \gets k+1$ 
		\State $r^h_k \gets \mSurfDisc{z^h_k}$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Consequences for a implementation}
So far we discussed the abstract numerical setting to solve the MSE. But when implementing this as a software solution, one has to consider several additional points, which especially involve the interface to the user.

We provide an overview of requirements for a potential implementation in figure~\ref{fig:requs}.

\begin{figure}[b]
	\centering
	\begin{tikzpicture}[node distance=4cm, auto]
	% Place nodes
	\node [block] (code) {Code/Solver};
	\node [cloud, left of=code, node distance=3cm] (in) {user input};
	\node [cloud, right of=code, node distance=3.5cm] (out) {output to user};
	%
	\path [line] (in) -- (code);
	\path [line] (code) -- (out);
	
	\node [cheap, below of=in](inputLegend) at(-3.95,0.3) {
		\small\begin{itemize}
		\item mesh size 
		\item boundary conditions 
		\item various tuning parameters
		\end{itemize}
		$\rightarrow$ user interface \\ via input file \normalsize
	};
	
	\node [cheap, below of=code] (solverLegend) at(0,0.25) {%
		\small\begin{itemize}
		\item discretization/grid 
		\item apply BC 
		\item determine Jacobian 
		\item solve linear systems 
		\item Newton iterations  \normalsize
		\end{itemize}
	};
	
	\node [cheap, below of = out] (outLegend) at(3.8, 0.3) {
		\small\begin{itemize}
		\item visualization of solution 
		\item convergence output (residual) 
		\item check validity of result
		\end{itemize}\normalsize
	};
	\end{tikzpicture}
	\caption{Requirements on a potential implementation}\label{fig:requs}
\end{figure}
We can divide communication with the user into user input and output to the user. A input interface can be realized e.g. as a GUI (graphical user interface) or as a input file (text file, that is parsed before/while running the simulations). Either realization serves the purpose to give the user a convenient possibility to set the main simulation parameters such as grid size and boundary conditions, as well as to set several tuning parameters like number of threads for parallel execution, initial guess.

The output to the user further is crucial in the sense that without giving the user any opportunity to perceive the result of the computation, there is no point in even computing anything. In terms of our application, it is required to provide the result in a form that can easily be visualized as a 3D-object. Additionally, one might want to offer a convenient validity-check and information about convergence.

Last but not least, a software implementation also needs to be able to perform all of the key steps to solve the MSE: It must allow for a discretizaion of the computational domain, and for setting the desired values on its boundary. Apart from implementing a suitable initial guess, inside a loop such as Algorithms~\ref{algo:standard-newton},~\ref{algo:laplace-newton} there are a few important tasks missing that require special consideration: Both computing the Jacobian $\nabla\mSurfDisc{\cdot}$ and solving the resulting linear systems are non-trivial necessities whose treatment will be discussed in more detail in the next chapter. 


\clearpage
% #########################################################################
\section{Implementation}
%##########################################################################
\subsection{Software/Solver Design}
\begin{figure}[H]
	\includegraphics[width=\textwidth]{./figs/ReportUML.pdf}
	\caption{UML Diagram showing the strucuture of the \textit{Minimal Surface} software.}
	\label{fig:UMLDiag}
\end{figure}

Figure \ref{fig:UMLDiag} shows the structure of the software, the classes and their associated functions, developed in order to solve the minimal surface problem. 
\begin{itemize}
	\item {\textbf{inputParser:} This class consists of functions necessary to read all the parameters required to control and execute the minimal surface software. This class takes in \texttt{params.in} which contains all the necessary parameters such as the \# of mesh elements, boundary conditions, option for the method of Jacobian computation etc... as an input to be parsed and processed. This class also depends on an external library, \texttt{ATMSP Expression Parser}, to parse the boundary conditions.}
	\item {\textbf{cartesianGrid:} This class consists of functions that help in the generation of a uniform cartesian mesh required for solving the FD problem. It also has functions required to return the $x$ and $y$ coordinates of a node required to apply the boundary conditions. It also provides function calls to allocate separate storage of boundary and inner node indices.}
	\item {\textbf{solver:} This class consists of functions required to solve the minimal surface problem using Newton-Raphson method. The function to apply the boundary conditions is specified inside this class. \texttt{ATMSP Expression Parser} library is again required for this as the library provides methods to parse the boundary condition given as a string expression inside \texttt{params.in} into an executable byte code. It also uses \texttt{DCO} library to compute the Jacobian required for the  Newton Raphson iterations. Separate function definitions are created to run the solver for different options of Jacobian such as Symbolic differentiation, AD by Hand and Matrix-free methods based on the option provided inside \texttt{params.in}}
	\item  {\textbf{postProcessor:} This class consists of functions designed to write the solver residual as a CSV file and the resulting minimal surface as a VTS file to be visualized inside \textit{Paraview}. To write the VTS file, the class uses the \texttt{VTK} library. The class also provides a function to compare the convergence of FD to analytical solution for the special case of a Scherk surface where the convergence rate is measured using both the \textit{Max Norm} as well as the  \textit{l2-Norm}.}
\end{itemize}
%##########################################################################
\subsection{Computing the Jacobian}
Within the Newton iterations, it is necessary to compute the inverse of the Jacobian of the discrete MSO and solve the resulting linear systems. One well-known approach is using Finite Differences to further discretise the discrete MSO, but it is not efficient when the mesh is very fine. As a result, Algorithmic Differentiation is used in this problem. We implemented this procedure following two different strategies:
\begin{itemize}
	\item Build the Jacobian by Algorithmic Differentiation (AD by hand) and compute its inverse by existing linear solvers.
	\item Do not explicitly build the Jacobian, but use existing AD libraries to multiply it with vectors. Substitute this into the self-implemented matrix-free linear solver.
\end{itemize}
To get an overview of the problem's dimensions and properties as well as to provide a solution to verify other approaches based on Algorithmic Differentiation, we first implemented a straightforward ("hard-coded") method to derive the required Jacobian $\nabla \mathtt{F}(z)$ by substituting Equ.~\refeq{eq:dx}-\refeq{eq:dxy} into Equ.~\refeq{eq:dMSE} and taking its derivatives of $\vect{z}$. As is indicated in section~\ref{section:Discretization}, the resulting Jacobian is sparse, so it is stored in \texttt{Eigen::SparseMatrix} format by \texttt{Eigen} library. Then solve $dz = \nabla\mSurfDisc{z^h_k}\inv y$ by Biconjugate gradient stabilized method (BiCGSTAB). It is used instead of BiCG or CG since $\nabla\mSurfDisc{z^h_k}$ is not symmetric in general.

\subsubsection{AD by hand}
As is shown in section~\ref{section:Discretization}, the discrete MSO of each gridpoint has only support from its 9 neighbors. Considering the derivation of the Jacobian as a loop of gridpoint-wise MSO over each gridpoint (which is parallelizable in nature), the gridpoint-wise discrete MSO for each gridpoint $(i,j)$ can be generalized as Equ.~\refeq{eq:mso}, where $h$ is a function which has 1 output $\mathtt{F}\rb{z[i][j]}$ and 9 inputs $z[i'][j'], i'\in\{i, i\pm1\}, j'\in\{j,j\pm1\}$. Given this function primal of $ \mathtt{F}: \mathbb{R}^n\rightarrow 1 $ (here $n=9$), it is advantageous to use a first-order adjoint model as Equ.~\refeq{eq:adj1}, \refeq{eq:adj2}. By seeding the output $y_{(1)}$ one can harvest the whole Jacobian row-wise, where one row of the Jacobian has only 9 non-zero entries. The function signature of the overloaded function is stated in Equ.~\refeq{eq:adj}. By the driver function in Equ.~\refeq{eq:adj.driver} (assume an inner gridpoint $(i,j)$ has index $idx$), one column of the Jacobian $a1\_z[idx][:]$ together with the primal $z[idx]$ is harvested for each gridpoint $(i,j)$.

\begin{equation}
\mathtt{F}\rb{z[i][j]} = h\rb{z[i'][j'], i'\in\{i, i\pm1\}, j'\in\{j,j\pm1\}}\label{eq:mso}
\end{equation}

\begin{align}
y &:= \mathtt{F}(\textbf{x})\label{eq:adj1} &&
\\
\textbf{x}_{(1)} &:= \nabla \mathtt{F}(\textbf{x})^{T} \cdot y_{(1)}\label{eq:adj2}
\end{align}

\begin{equation}
\rb{y, \textbf{x}_{(1)}} := \mathtt{F}_{(1)}\rb{\textbf{x}, y_{(1)}}\qquad \mathtt{F}_{(1)}: \rb{\mathbb{R}^n\times 1}\rightarrow \rb{1\times\mathbb{R}^n}\label{eq:adj}
\end{equation}

\begin{equation}
\rb{z[idx],\ a1\_z[idx][:]} = \mathtt{F}\rb{z[:],\ a1\_Fz[idx]}
\label{eq:adj.driver}
\end{equation}

The row-wise derivation of the Jacobian inside a loop over all inner gridpoints can be decomposed into a single assignment code (SAC). Example of code snippets for each inner gridpoint if index $i$ are as Code~\ref{lst:prsac}, \ref{lst:adsac}. Because of the sparsity of the Jacobian, the computational complexity for each inner gridpoint is a constant. Therefore, the total computational complexity is $\mathcal{O}(N^2)$ for $N^2$ gridpoints (or to say, $N^2$ rows of the Jacobian).

Same as the "hard-coded" method, we store the derived Jacobian in \texttt{Eigen::SparseMatrix} format and solve the linear system by BiCGSTAB.

Results have proven that we apply Algorithmic Differentiation to the problem and get a feasible result. However, since it is "by hand", it has certain limitations. If the problem is more complex, it is inefficient, or even impossible to manually derive it step by step. A more general approach is still needed to automatically derive the Jacobian without manual computation.

\lstset{backgroundcolor=\color{gray!20}, language=C++, basicstyle=\ttfamily, breaklines=true}

\noindent
\begin{minipage}[t]{0.45\linewidth}
\begin{lstlisting}[caption={Adjoint SAC Code for inner gridpoint $i$ (primal/forward)},label=lst:prsac,language=C++]
// d(...) = fd(x)
dx = (z[i+1]-z[i-1])/(2*h);
dy = (z[i+N]-z[i-N])/(2*h);
dxx, dxy, dyy = ...

// intermidiates v1, v2, v3
v1 = (1+pow(dx,2))*dyy;
v2 = -2*dx*dy*dxy;
v3 = (1+pow(dy,2))*dxx;

// result
Fz[i] = v1+v2+v3;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}[t]{0.53\linewidth}
\begin{lstlisting}[caption={Adjoint SAC Code for inner gridpoint $i$ (adjoint/reverse)},label=lst:adsac,language=C++]
(initialize everything to zero)
// seed a1_Fz
a1_Fz[i] = 1.0;

// result Fz[i] = v1+v2+v3
a1_v1 = a1_Fz[i];
a1_v2 = a1_Fz[i];
a1_v3 = a1_Fz[i];

// intermidiates v1, v2, v3
a1_dy = dxx*2*dy*a1_v3;
a1_dxx = (1+pow(dy,2))*a1_v3;
...

// d(...) = fd(x)
a1_z[i][i+1] = a1_dx/(2*h)+...;
a1_z[i][i-1] = a1_dx/(-2*h)+...;
...
\end{lstlisting}
\end{minipage}

\subsubsection{matrix-free linear solver with \texttt{dco\_c++}}
\texttt{dco\_c++} is a library that implements Algorithmic Differentiation by overloading in C++. It provides the same function as AD by hand in terms of harvesting the Jacobian. However, if all manual computation is eliminated, it is unable to exploit the sparsity of the Jacobian, so we need $\mathcal{O}(N^4)$ space to store it, which for large $N$ is impossible. Therefore, we implemented a matrix-free linear solver inside Newton's method as described in Algorithm~\ref{algo:matfree-newton}, which does not require explicitly building the Jacobian, but instead apply the vector tangent code overloaded by \texttt{dco\_c++} as a matrix-vector multiplication tool which multiplies the Jacobian with the seed to harvest the result.

In general, the MSO is an operator of $ \mathtt{F}: \mathbb{R}^{N\times N}\rightarrow \mathbb{R}^{N\times N} $, which has the same computational complexity for vector tangent mode and vectors adjoint mode. We choose vector tangent mode in our implementation.

\begin{algorithm}[H]
	\caption{Newton's method}\label{algo:matfree-newton}
	\begin{algorithmic}
		\State $\vect{y} \gets \mathtt{F}(\vect{z})$ 
		\While{$\lVert \vect{y} \rVert > \textsc{tol}$} 
		\State $\vect{A} = \nabla \mathtt{F}(\vect{z})$ 
		\State \color{blue} $\vect{dz} = \vect{A}\inv \vect{y}$  \normalcolor \Comment{Instead of assembling the whole $\nabla \mathtt{F}$ above, do this matrix-free}
		\State $\vect{z} \gets \vect{z} + \vect{dz}$
		\State $\mathtt{res} \gets \mathtt{F}(\vect{z})$ 
		\EndWhile
	\end{algorithmic}
\end{algorithm}

The first-order vector tangent mode for $\mathtt{F}: \mathbb{R}^{N\times N}\rightarrow \mathbb{R}^{N\times N} $ is defined as Equ.~\refeq{eq:tg1}, \refeq{eq:tg2}, and its function signature is defined in Equ.~\refeq{eq:tg}. By substituting Equ.~\refeq{eq:tg1} into an existing linear solver (for example BiCGSTAB, as we implemented in Algorithm~\ref{algo:bicgstab}), $\vect{dz} = \vect{A}\inv \vect{y}$ is solved by this iterative approach.

The advantage of the matrix-free linear solver with \texttt{dco\_c++} is that it does not require any prior knowledge of the operator. Given any primal \texttt{y = F(z)} where $ \mathtt{F}: \mathbb{R}^{N\times N}\rightarrow \mathbb{R}^{N\times N} $, only several lines of code is needed for the vector tangent mode by overloading, as is shown in Code~\ref{lst:vot}.
\begin{lstlisting}[caption={Vectors of Tangents by Overloading},label=lst:vot,language=C++]
// activate
std::vector<ADtype> z_(std::begin(z),std::end(z));
// seed
for (auto& i: grid.innerNodeList) 
dco::derivative(z_)[i] = dz[i];
// compute
y_ = F(z_); // overloaded
// harvest
dy = dco::derivative(y_);
\end{lstlisting}

\begin{align}
\textbf{y} &:= \mathtt{F}(\textbf{x})\label{eq:tg1} &&
\\
Y^{(1)} &:= \nabla \mathtt{F}(\textbf{x}) \cdot X^{(1)}\label{eq:tg2}
\end{align}
\begin{equation}
\rb{\textbf{y}, Y^{(1)}} := \mathtt{F}^{(1)}\rb{\textbf{x}, X^{(1)}}\qquad \mathtt{F}^{(1)}: \rb{\mathbb{R}^{N\times N}\times \mathbb{R}^{N\times N}}\rightarrow \rb{\mathbb{R}^{N\times N}\times\mathbb{R}^{N\times N}}\label{eq:tg}
\end{equation}

\begin{algorithm}[H]
	\caption{Matrix-free BiCGSTAB to get $\vect{dz}$ (adapted from \cite{naumann})}\label{algo:bicgstab}
	\small{	\begin{algorithmic}
			\State $\vect{z}^{(1)} \gets \vect{dz}$ 
			\State \color{blue}$(\vect{y}, \vect{y}^{(1)}) \gets \mathtt{F}^{(1)} (\vect{z}, \vect{z}^{(1)}) $ \normalcolor \Comment{DCO}
			\State $\vect{p} \gets -\vect{y} -\vect{y}^{(1)}$
			\State $\vect{r} \gets \vect{p}, \vect{r_0}\gets \vect{r},\vect{p}\gets \vect{0},\vect{v}\gets \vect{0}, \rho\gets1,\alpha\gets1,\omega\gets1 $
			\While{$\lVert \vect{r} \rVert > \textsc{tol}$} 
			\State $\rho_{\text{new}} = \big(\vect{r_0},\vect{r}\big)$
			\State $\beta \gets \rho_{\text{new}}/\rho \cdot \alpha/\omega$
			\State $\rho \gets \rho_{\text{new}}$
			\State $\vect{p}\gets \vect{r}+\beta(\vect{p}-\omega \vect{v})$
			\State $\vect{z}^{(1)} \gets \vect{p}$
			\State \color{blue}$(\vect{y}, \vect{y}^{(1)}) \gets \mathtt{F}^{(1)} (\vect{z}, \vect{z}^{(1)}) $ \normalcolor \Comment{DCO}
			\State $\vect{v}\gets \vect{y}^{(1)}$
			\State $\alpha \gets \rho/\big(\vect{r_0},\vect{v}\big)$ 
			\State $\vect{dz} \gets \vect{dz} + \alpha \vect{p}$
			\If{$\lVert \mathtt{F}(\vect{z}+\vect{dz}) \rVert < \textsc{tol} $}
			\State abort \Comment{Converged at intermediate level}
			\EndIf
			\State $\vect{s}\gets \vect{r} - \alpha \vect{v}$
			\State $\vect{z}^{(1)} \gets \vect{s}$
			\State \color{blue}$(\vect{y}, \vect{y}^{(1)}) \gets \mathtt{F}^{(1)} (\vect{z}, \vect{z}^{(1)}) $ \normalcolor \Comment{DCO}
			\State $\omega \gets \big(\vect{y}^{(1)}, \vect{s} \big) / \big(\vect{y}^{(1)}, \vect{y}^{(1)} \big)$
			\State $\vect{dz} \gets \vect{dz} + \omega \vect{s}$
			\State $\vect{r} \gets \vect{s} - \omega \vect{y}^{(1)}$
			\EndWhile
			\State $\vect{z} \gets \vect{z} + \vect{dz}$
	\end{algorithmic}}
\end{algorithm}

There are also drawbacks of the matrix-free linear solver. One is already noted, that it is not as flexible as AD by hand to exploit the structure of the Jacobian (for example, sparsity) to reduce computation and space complexity. It remains an interesting topic for researches. Another issue is that the mostly sequential matrix-free solver is not easy to parallelize. Although there exist some better implementations than ours, it is not user-friendly to require any implementation of linear solvers. For example, if the user knows the Jacobian is symmetric, a CG method which is faster than BiCGSTAB must be implemented from scratch.

\subsection{Solving linear systems}

	\textbf{I tested a bit now, and will push also to the code, timings ideally after that:
	seems like sparseLU is faster for getting the initial guess, so I will implement that.\\
	for the loops, bicgstab is still advantageous as we can greatly reduce TOL\_linsolver.}
\normalfont
\subsection{Testing}
@Praveen
here the figure as in the presentation, short explanation (basically what you said)

maybe 1-2 results of tests, short explanation

checking of validity by means of scherk, present a result here

% #########################################################################
\section{Results}
@Praveen you can refer to Scherk from General background, 
results for the 2 testcases, some timings (for all solvers for a choice of $\#$gridpoints, not too many), scaling of the three (fix $\#$gridpoints to sth $>500$ for jacOption 0,1 and maybe max. 200-300 for 2, and run for 1,2,4,8,16 threads)
once timings are implemented, you can also ask us to help determine times (consistent TOL's etc...)


% #########################################################################
\section{Conclusions and outlook}
pretty much what we had in the presentation

@all, feel free to add ideas here, I would merge it together in the end

% #########################################################################

%
\bibliographystyle{apalike}
\bibliography{bibliography}
\clearpage
\appendix
\section{User manual}
This section provides a short description on how to run the code. The software accompanying this report is self-contained and guaranteed to run on the RWTH cluster, using either the current versions of the Intel (19) or GCC (9) compiler.

\subsection{Compilation}
To compile the code, open a terminal inside \texttt{source/}. 
\lstset{backgroundcolor=\color{gray!20}, language=bash, basicstyle=\ttfamily}
\begin{lstlisting}
	cd build 
	cmake .. 
	make
\end{lstlisting}
The standard setup is the CMAKE compile option \texttt{Release} admitting some compiler optimization. If some additional debug information is desired, change the option to \texttt{debug} inside \texttt{CMakeLists.txt}.
\subsection{Running a simulation}
To run a simulation, go to the folder testcases or create a new folder with a valid \texttt{params.in}-file.

Inside this \texttt{params.in}-file, one can find the options to set with descriptions.

\noindent To run the a simulation, execute the following, once the input file is set up:
\lstset{backgroundcolor=\color{gray!20}, language=bash, basicstyle=\ttfamily, breaklines=true}
\begin{lstlisting}
	cd <folder with params.in> 
	export NAG_KUSARI_FILE=<path to nag_key.txt>        # needed if jacobianOpt=2 
	ln -s <path to executable minSurf in build/> .      # symbolic link for convenient execution 
	./minSurf
\end{lstlisting}
Now, the resulting output can be inspected by opening the \texttt{.vts}-files inside \texttt{surfaces/} using Paraview\footnote{to use this on the cluster, execute \texttt{module load GRAPHICS paraview} beforehand.}. If one is interesting in the convergence, one can find the residual data over iteration inside \texttt{residual/}, which can be also plotted using Paraview. 


\end{document}          
